{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2435ed-0175-4483-83e8-9b81b8b5b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración Inicial y Supresión de Warnings\n",
    "\n",
    "# En ocasiones, algunas bibliotecas pueden generar advertencias (warnings) que no son críticas\n",
    "# para la ejecución del código pero pueden clutter la salida.\n",
    "# Esta sección las suprime para mantener una salida más limpia durante el desarrollo.\n",
    "# Es una buena práctica revisar los warnings en fases finales para asegurar que no se ignora nada importante.\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "print(\"Warnings suprimidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bd962-cf53-46ec-bfca-6a8e3e0576a1",
   "metadata": {},
   "source": [
    "### Configuración Inicial y Supresión de Warnings - Explicación\n",
    "\n",
    "**Propósito de la Celda:**\n",
    "\n",
    "Esta celda inicial se encarga de configurar el entorno para una ejecución más limpia, específicamente suprimiendo las advertencias (warnings) que Python o sus bibliotecas puedan generar.\n",
    "\n",
    "**Detalles del Código:**\n",
    "\n",
    "*   `def warn(*args, **kwargs): pass`: Se define una función vacía llamada `warn`.\n",
    "*   `import warnings`: Se importa el módulo `warnings` de Python, que es el encargado de manejar cómo se muestran las advertencias.\n",
    "*   `warnings.warn = warn`: Se sobrescribe la función `warn` original del módulo `warnings` con nuestra función vacía. Esto tiene el efecto de que, cuando una biblioteca intente emitir una advertencia, llamará a nuestra función `warn` que no hace nada, suprimiendo así la advertencia en la salida.\n",
    "*   `print(\"Warnings suprimidos.\")`: Un mensaje para confirmar que la configuración se ha aplicado.\n",
    "\n",
    "**¿Por qué suprimir warnings?**\n",
    "\n",
    "Durante el desarrollo y la exploración, las bibliotecas pueden emitir warnings sobre futuras deprecaciones de funciones, conversiones de tipos implícitas, o comportamientos que podrían cambiar. Si bien estos warnings son informativos, pueden hacer que la salida del notebook sea extensa y difícil de leer. Suprimirlos temporalmente puede ayudar a enfocarse en los resultados principales.\n",
    "\n",
    "**Consideraciones Importantes:**\n",
    "\n",
    "*   **Uso Cauteloso:** Es una buena práctica no ignorar los warnings permanentemente. Una vez que el código esté más estable o antes de una revisión final, es recomendable comentar esta sección para ver si hay advertencias importantes que necesiten ser atendidas (por ejemplo, el uso de una función que será eliminada en futuras versiones de una biblioteca).\n",
    "*   **Entornos Específicos:** En algunos entornos de producción o para depuración, es preferible ver todos los warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3657b-b836-4aaf-baae-5ae2dd83c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Importación de Bibliotecas Principales\n",
    "\n",
    "import pandas as pd               # Para manipulación y análisis de datos tabulares (DataFrames)\n",
    "import numpy as np                # Para operaciones numéricas, especialmente con arrays\n",
    "import matplotlib.pyplot as plt   # Para visualizaciones estáticas básicas\n",
    "import seaborn as sns             # Para visualizaciones estadísticas más atractivas y complejas, basadas en matplotlib\n",
    "import plotly.express as px       # Para visualizaciones interactivas y dinámicas\n",
    "import plotly.graph_objects as go # Para mayor control sobre las visualizaciones de Plotly\n",
    "\n",
    "# Configuración para que los gráficos de matplotlib se muestren directamente en el notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuraciones opcionales para mejorar la estética de los gráficos de Seaborn/Matplotlib\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6) # Tamaño por defecto de las figuras de matplotlib\n",
    "\n",
    "print(\"Bibliotecas principales importadas y configuraciones aplicadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0567ed-1e16-473e-bc1e-fb06c0c8faf7",
   "metadata": {},
   "source": [
    "### Importación de Bibliotecas Principales - Explicación\n",
    "\n",
    "**Propósito de la Celda:**\n",
    "\n",
    "Esta celda es fundamental ya que carga todas las bibliotecas (o \"librerías\") de Python que necesitaremos para llevar a cabo nuestro análisis de datos, visualización y modelado.\n",
    "\n",
    "**Detalles del Código y Bibliotecas Importadas:**\n",
    "\n",
    "*   **Instalación de Paquetes (para entornos específicos):**\n",
    "    *   `try...except ImportError`: Este bloque intenta importar `piplite`.\n",
    "    *   `import piplite`: `piplite` es un gestor de paquetes específico para entornos basados en WebAssembly como JupyterLite o Pyodide (el kernel que parece estar usando tu notebook original).\n",
    "    *   `await piplite.install(['seaborn', 'plotly'])`: Si `piplite` está disponible, se utiliza para asegurar que las bibliotecas `seaborn` y `plotly` (que usaremos para visualizaciones avanzadas e interactivas) estén instaladas en ese entorno.\n",
    "    *   **Nota para Entornos Locales:** Si estás ejecutando este notebook en un entorno Python local (como Anaconda, VS Code con un kernel de Python, o Jupyter Notebook/Lab estándar), `piplite` no será necesario. En ese caso, debes asegurarte de haber instalado estas bibliotecas previamente usando `pip` o `conda` (ej. `pip install pandas numpy matplotlib seaborn plotly scikit-learn`).\n",
    "\n",
    "*   **Bibliotecas Estándar de Ciencia de Datos:**\n",
    "    *   `import pandas as pd`: **Pandas** es la herramienta esencial para trabajar con datos estructurados (tablas). Proporciona estructuras de datos como el `DataFrame` y funciones para leer, escribir, limpiar, transformar y analizar datos. Se importa con el alias `pd`.\n",
    "    *   `import numpy as np`: **NumPy** (Numerical Python) es la base para la computación numérica en Python. Proporciona soporte para arrays multidimensionales, matrices y una amplia gama de funciones matemáticas para operar sobre ellos. Pandas se basa en NumPy. Se importa con el alias `np`.\n",
    "    *   `import matplotlib.pyplot as plt`: **Matplotlib** es una biblioteca fundamental para crear visualizaciones estáticas, como gráficos de líneas, barras, histogramas, etc. `pyplot` es un módulo dentro de Matplotlib que proporciona una interfaz similar a MATLAB. Se importa con el alias `plt`.\n",
    "    *   `import seaborn as sns`: **Seaborn** es una biblioteca de visualización de datos basada en Matplotlib. Proporciona una interfaz de más alto nivel para dibujar gráficos estadísticos atractivos e informativos. Se importa con el alias `sns`.\n",
    "    *   `import plotly.express as px` y `import plotly.graph_objects as go`: **Plotly** es una biblioteca para crear visualizaciones interactivas y de calidad de publicación. `plotly.express` (alias `px`) ofrece una interfaz concisa y de alto nivel (similar a Seaborn) para gráficos comunes, mientras que `plotly.graph_objects` (alias `go`) proporciona más control para gráficos personalizados y complejos.\n",
    "\n",
    "*   **Configuraciones de Visualización:**\n",
    "    *   `%matplotlib inline`: Es un \"comando mágico\" de IPython/Jupyter que asegura que los gráficos generados por Matplotlib se muestren directamente dentro del notebook, en lugar de en una ventana separada.\n",
    "    *   `sns.set_style(\"whitegrid\")`: Establece un estilo visual predeterminado para los gráficos de Seaborn (y por extensión Matplotlib), en este caso, un fondo blanco con rejilla.\n",
    "    *   `plt.rcParams['figure.figsize'] = (10, 6)`: Configura el tamaño por defecto (ancho de 10 pulgadas, alto de 6 pulgadas) para las figuras generadas por Matplotlib.\n",
    "\n",
    "Esta celda prepara el terreno con las herramientas necesarias antes de comenzar a trabajar con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522dcfea-0ae1-400b-b648-78c697030d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de Descarga de Datos\n",
    "\n",
    "# Esta función es útil si el notebook se ejecuta en un entorno donde los archivos\n",
    "# necesitan ser descargados desde una URL, como Google Colab o Pyodide.\n",
    "# Si el archivo CSV ya está localmente en el mismo directorio que el notebook,\n",
    "# esta función y su llamada pueden ser omitidas.\n",
    "\n",
    "from pyodide.http import pyfetch # Específico para el entorno Pyodide\n",
    "\n",
    "async def download(url, filename):\n",
    "    \"\"\"\n",
    "    Descarga un archivo desde una URL y lo guarda localmente.\n",
    "    Utiliza pyfetch, adecuado para entornos como Pyodide.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await pyfetch(url)\n",
    "        if response.status == 200:\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(await response.bytes())\n",
    "            print(f\"Archivo '{filename}' descargado exitosamente desde {url}\")\n",
    "        else:\n",
    "            print(f\"Error al descargar {url}: Código de estado {response.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error durante la descarga: {e}\")\n",
    "\n",
    "# Confirmación de que la función está definida\n",
    "print(\"Función 'download' definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b966e-070b-4a9d-818e-a23dc68dc9a8",
   "metadata": {},
   "source": [
    "### Función de Descarga de Datos - Explicación\n",
    "\n",
    "**Propósito de la Celda:**\n",
    "\n",
    "Esta celda define una función asíncrona llamada `download`. Su propósito es descargar un archivo desde una URL especificada y guardarlo localmente con un nombre de archivo también especificado.\n",
    "\n",
    "**Detalles del Código:**\n",
    "\n",
    "*   `from pyodide.http import pyfetch`: Esta importación es específica para el entorno **Pyodide**, que es el que se utiliza en JupyterLite (y el kernel Python (Pyodide) que indicaste en los metadatos de tu notebook original). `pyfetch` es una función que permite realizar solicitudes HTTP (como descargar archivos) en este entorno basado en WebAssembly.\n",
    "*   `async def download(url, filename):`:\n",
    "    *   `async def`: Define la función como asíncrona. Esto es necesario porque `pyfetch` y `response.bytes()` son operaciones asíncronas (no bloqueantes).\n",
    "    *   `url`: Parámetro que representa la URL del archivo a descargar.\n",
    "    *   `filename`: Parámetro que representa el nombre con el que se guardará el archivo localmente.\n",
    "*   `response = await pyfetch(url)`: Realiza la solicitud HTTP a la `url` y espera (`await`) la respuesta.\n",
    "*   `if response.status == 200:`: Verifica si la solicitud fue exitosa (un código de estado HTTP 200 significa \"OK\").\n",
    "*   `with open(filename, \"wb\") as f:`: Abre un archivo localmente en modo escritura binaria (`\"wb\"`). El uso de `with` asegura que el archivo se cierre correctamente incluso si ocurren errores.\n",
    "*   `f.write(await response.bytes())`: Escribe el contenido de la respuesta (los bytes del archivo descargado) en el archivo local.\n",
    "*   Los mensajes `print` informan sobre el éxito o el fracaso de la descarga.\n",
    "*   El bloque `try...except` maneja posibles errores durante el proceso de descarga.\n",
    "\n",
    "**Relevancia y Contexto:**\n",
    "\n",
    "*   **Entornos Web/Remotos:** Esta función es particularmente útil cuando el notebook se ejecuta en entornos donde no se tiene acceso directo al sistema de archivos local para colocar el dataset, como Google Colab (que usaría `!wget` o `gdown`), JupyterLite, o cualquier otro servicio en la nube donde los datos deban ser traídos desde una fuente externa.\n",
    "*   **Entornos Locales:** Si el archivo CSV (`kc_house_data_NaN.csv`) ya se encuentra en el mismo directorio que tu archivo Jupyter Notebook cuando lo ejecutas localmente, esta función `download` y la celda que la llama (Celda 5) no serían estrictamente necesarias, ya que podrías cargar el archivo directamente con `pd.read_csv(\"nombre_del_archivo.csv\")`. Sin embargo, mantenerla hace el notebook más portable y reproducible en diferentes entornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e4da1-e755-49d7-9f1c-487cb45b2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la URL del dataset\n",
    "\n",
    "filepath = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\n",
    "file_name_local = \"housing.csv\"\n",
    "\n",
    "# Intentamos detectar si estamos en Pyodide\n",
    "import sys\n",
    "\n",
    "IN_PYODIDE = sys.platform == \"emscripten\"\n",
    "\n",
    "if IN_PYODIDE:\n",
    "    # Código para Pyodide / JupyterLite\n",
    "    try:\n",
    "        from pyodide.http import pyfetch\n",
    "        import asyncio\n",
    "\n",
    "        async def download(url, filename):\n",
    "            response = await pyfetch(url)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(await response.bytes())\n",
    "\n",
    "        await download(filepath, file_name_local)\n",
    "        print(f\"Archivo descargado exitosamente en Pyodide como '{file_name_local}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al intentar descargar en Pyodide: {e}\")\n",
    "        print(f\"Puedes intentar descargar manualmente desde:\\n{filepath}\")\n",
    "else:\n",
    "    # Código para entorno local (Jupyter, VS Code, etc.)\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    if os.path.exists(file_name_local):\n",
    "        print(f\"El archivo '{file_name_local}' ya existe localmente.\")\n",
    "    else:\n",
    "        try:\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(filepath, file_name_local)\n",
    "            print(f\"Archivo descargado exitosamente en entorno local como '{file_name_local}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error al intentar descargar en entorno local: {e}\")\n",
    "            print(f\"Puedes descargar manualmente desde:\\n{filepath}\")\n",
    "\n",
    "# Intentamos cargar el archivo en un DataFrame\n",
    "try:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(file_name_local)\n",
    "    print(f\"\\nDatos cargados exitosamente. Dimensiones: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNo se pudo cargar el archivo '{file_name_local}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b942a5-6766-4735-85c5-39c1edd6496e",
   "metadata": {},
   "source": [
    "### Descarga del Archivo de Datos - Explicación\n",
    "\n",
    "**Propósito de la Celda:**\n",
    "\n",
    "El objetivo principal de esta celda es obtener el archivo de datos (`.csv`) desde la URL especificada en la celda anterior (`filepath`) y guardarlo en el entorno de ejecución local del notebook bajo un nombre más manejable (`housing.csv`).\n",
    "\n",
    "**Detalles del Código:**\n",
    "\n",
    "*   `file_name_local = \"housing.csv\"`: Se define una variable que contendrá el nombre que queremos darle al archivo una vez descargado y guardado localmente.\n",
    "*   `try...except NameError...except Exception...`: Se utiliza un bloque `try-except` para manejar la descarga.\n",
    "    *   `await download(filepath, file_name_local)`: Se llama a la función `download` (definida en la Celda 3).\n",
    "        *   `await`: Es necesario porque `download` es una función asíncrona.\n",
    "        *   `filepath`: Es la URL del archivo fuente (definida en la Celda 4).\n",
    "        *   `file_name_local`: Es el nombre con el que se guardará el archivo en el sistema de archivos del entorno donde se ejecuta el notebook.\n",
    "    *   **Manejo de Errores (`except` blocks):**\n",
    "        *   `except NameError as e`: Este bloque se activa si la función `download` o `pyfetch` no están definidas. Esto es común si se ejecuta el notebook en un entorno Python local estándar donde Pyodide no está presente. En tal caso, se imprime un mensaje informativo indicando al usuario que debe asegurarse de que el archivo esté disponible manualmente.\n",
    "        *   `except Exception as e`: Captura cualquier otro error que pueda ocurrir durante el intento de descarga (problemas de red, URL incorrecta, etc.) e informa al usuario.\n",
    "*   `print(...)`: Un mensaje final confirma que se intentó la descarga y qué nombre de archivo se usará para cargar los datos.\n",
    "\n",
    "**Funcionamiento y Relevancia:**\n",
    "\n",
    "*   **Entornos Pyodide/JupyterLite:** En estos entornos, el `await download(...)` ejecutará la descarga usando `pyfetch` y guardará `housing.csv` en el sistema de archivos virtual del navegador.\n",
    "*   **Entornos Locales (Anaconda, VS Code, etc.):**\n",
    "    *   Si la función `download` (y `pyfetch`) no están disponibles, el bloque `except NameError` se ejecutará.\n",
    "    *   En este escenario, se asume que el usuario ya ha descargado el archivo `kc_house_data_NaN.csv`, lo ha renombrado (opcionalmente) a `housing.csv`, y lo ha colocado en el mismo directorio donde se está ejecutando el notebook. O bien, en la siguiente celda, se podría usar la URL directamente con `pd.read_csv()` si la conexión a internet es estable, aunque descargar una vez es más eficiente.\n",
    "\n",
    "Esta celda actúa como un puente para asegurar que el conjunto de datos esté accesible para ser cargado por Pandas en la siguiente etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0aab5-5456-4b7c-ac6b-47a917838e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de Datos en un DataFrame de Pandas\n",
    "\n",
    "# Una vez que el archivo \"housing.csv\" está disponible localmente (ya sea descargado\n",
    "# por la celda anterior o colocado manualmente), utilizamos la biblioteca Pandas\n",
    "# para leer este archivo CSV y cargarlo en una estructura de datos tabular llamada DataFrame.\n",
    "# El DataFrame es la principal herramienta con la que trabajaremos para analizar y manipular los datos.\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_name_local) # file_name_local fue definido como \"housing.csv\"\n",
    "    print(f\"Archivo '{file_name_local}' cargado exitosamente en un DataFrame.\")\n",
    "    print(f\"El DataFrame tiene {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{file_name_local}' no fue encontrado.\")\n",
    "    print(\"Por favor, asegúrate de que el archivo fue descargado correctamente o está en el directorio de trabajo.\")\n",
    "    print(f\"Puedes intentar descargarlo de nuevo o manualmente desde: {filepath}\")\n",
    "    df = pd.DataFrame() # Crear un DataFrame vacío para evitar errores en celdas posteriores si la carga falla\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al leer el archivo CSV: {e}\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Mostramos las primeras filas para una verificación rápida si la carga fue exitosa\n",
    "if not df.empty:\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3c194-4241-4aff-a8ed-1fc66b77fcdc",
   "metadata": {},
   "source": [
    "### Carga de Datos en un DataFrame de Pandas - Explicación\n",
    "\n",
    "**Propósito de la Celda:**\n",
    "\n",
    "El objetivo de esta celda es leer el archivo de datos (`housing.csv`, que se obtuvo en la celda anterior) y cargarlo en una estructura de datos de **Pandas** llamada `DataFrame`. Un DataFrame es esencialmente una tabla, similar a una hoja de cálculo o una tabla de base de datos, y es la estructura fundamental para la manipulación y análisis de datos en Python con Pandas.\n",
    "\n",
    "**Detalles del Código:**\n",
    "\n",
    "*   `try...except FileNotFoundError...except Exception...`: Se utiliza un bloque `try-except` para manejar la carga del archivo de manera robusta.\n",
    "    *   `df = pd.read_csv(file_name_local)`:\n",
    "        *   `pd.read_csv()`: Es la función de Pandas utilizada para leer archivos CSV.\n",
    "        *   `file_name_local`: Es la variable que contiene el nombre del archivo CSV a leer (definido como `\"housing.csv\"` en la celda anterior).\n",
    "        *   El resultado de esta función (el contenido del archivo CSV leído y estructurado) se asigna a la variable `df`. A partir de ahora, `df` será nuestro DataFrame principal.\n",
    "    *   `print(...)`: Se imprimen mensajes para informar sobre el resultado de la carga y las dimensiones del DataFrame (número de filas y columnas) usando `df.shape`.\n",
    "    *   **Manejo de Errores:**\n",
    "        *   `except FileNotFoundError`: Si el archivo `housing.csv` no se encuentra en el directorio de trabajo (por ejemplo, si la descarga falló y no se colocó manualmente), se captura este error y se informa al usuario. Se crea un `df` vacío para que las celdas siguientes no fallen inmediatamente por una variable `df` no definida, aunque el análisis no podrá continuar sin datos.\n",
    "        *   `except Exception as e`: Captura cualquier otro error que pueda ocurrir durante la lectura del archivo (formato incorrecto, problemas de permisos, etc.).\n",
    "*   `if not df.empty: display(df.head())`: Si el DataFrame `df` no está vacío (es decir, la carga fue exitosa o al menos se creó un DataFrame), se muestra una vista previa de las primeras 5 filas usando `df.head()`. Esto permite una verificación visual rápida de que los datos se han cargado correctamente y tienen la estructura esperada.\n",
    "\n",
    "**Importancia del DataFrame:**\n",
    "\n",
    "Una vez que los datos están en un DataFrame de Pandas, podemos:\n",
    "*   Inspeccionar los datos (ver tipos de datos, valores faltantes, estadísticas descriptivas).\n",
    "*   Limpiar y preprocesar los datos (manejar valores faltantes, convertir tipos, eliminar duplicados).\n",
    "*   Transformar y manipular los datos (filtrar filas, seleccionar columnas, crear nuevas características).\n",
    "*   Realizar análisis exploratorio de datos (EDA) y visualizaciones.\n",
    "*   Preparar los datos para modelos de machine learning.\n",
    "\n",
    "Esta celda marca el inicio formal del trabajo con el conjunto de datos dentro de nuestro entorno de análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba4d7a-c0c9-476a-9af7-637248a6aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista General del DataFrame\n",
    "print(\"Primeras 5 filas del DataFrame:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nInformación general del DataFrame (tipos de datos, nulos):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nResumen estadístico de las columnas numéricas:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21634df-6e8e-4c13-a5e1-e1650c3fdf05",
   "metadata": {},
   "source": [
    "### Vista General del DataFrame - Explicación\n",
    "\n",
    "Esta celda nos proporciona una primera toma de contacto con nuestros datos.\n",
    "\n",
    "1.  **`df.head()`**: Muestra las primeras 5 filas del DataFrame. Esto es útil para ver la estructura de los datos, los nombres de las columnas y algunos valores de ejemplo.\n",
    "2.  **`df.info()`**: Proporciona un resumen conciso del DataFrame, incluyendo:\n",
    "    *   El tipo de dato de cada columna (Dtype).\n",
    "    *   El número de valores no nulos en cada columna. Esto es crucial para identificar columnas con datos faltantes.\n",
    "    *   El uso de memoria del DataFrame.\n",
    "3.  **`df.describe()`**: Genera estadísticas descriptivas para las columnas numéricas. Esto incluye:\n",
    "    *   `count`: Número de valores no nulos.\n",
    "    *   `mean`: Media.\n",
    "    *   `std`: Desviación estándar.\n",
    "    *   `min`: Valor mínimo.\n",
    "    *   `25%`, `50%`, `75%`: Percentiles (cuartiles), donde el 50% es la mediana.\n",
    "    *   `max`: Valor máximo.\n",
    "\n",
    "**Observaciones Iniciales:**\n",
    "*   Vemos columnas como `Unnamed: 0` e `id` que podrían no ser útiles para el modelado y podrían eliminarse.\n",
    "*   La columna `date` es de tipo `object` (probablemente una cadena de texto) y necesitará ser convertida a un formato de fecha/hora si queremos extraer información temporal.\n",
    "*   `bedrooms` y `bathrooms` tienen menos valores no nulos que el total de filas, indicando la presencia de NaNs que ya se identificaron en el notebook original.\n",
    "*   Las estadísticas descriptivas nos dan una idea de la escala y distribución de cada variable numérica. Por ejemplo, `price` tiene una gran desviación estándar, lo que sugiere una amplia gama de precios.\n",
    "\n",
    "Esta información inicial es fundamental para planificar los siguientes pasos de limpieza y preprocesamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf647f-3022-4276-9640-1f1cbc74770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de Columnas Irrelevantes\n",
    "\n",
    "# Columnas a eliminar\n",
    "columns_to_drop = [\"id\", \"Unnamed: 0\"]\n",
    "\n",
    "# Verificamos si las columnas existen antes de intentar eliminarlas\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {existing_columns_to_drop}\")\n",
    "else:\n",
    "    print(\"Las columnas especificadas para eliminar no se encontraron o ya fueron eliminadas.\")\n",
    "\n",
    "print(\"\\nPrimeras filas después de eliminar columnas:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nResumen estadístico después de eliminar columnas:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5e27c-ec0a-47d0-93aa-81927eed6410",
   "metadata": {},
   "source": [
    "### Eliminación de Columnas Irrelevantes - Explicación\n",
    "\n",
    "En esta celda, procedemos a eliminar columnas que no aportan valor predictivo a nuestro modelo de precios de viviendas o que son redundantes.\n",
    "\n",
    "1.  **Identificación de Columnas:**\n",
    "    *   `id`: Es un identificador único para cada casa y no tiene relación intrínseca con el precio. Mantenerlo podría confundir al modelo.\n",
    "    *   `Unnamed: 0`: Esta columna a menudo aparece cuando un DataFrame se guarda en CSV con el índice y luego se vuelve a cargar. Esencialmente, es un índice duplicado y no es útil.\n",
    "\n",
    "2.  **Proceso de Eliminación:**\n",
    "    *   Se define una lista `columns_to_drop`.\n",
    "    *   Se verifica si estas columnas realmente existen en el DataFrame actual para evitar errores si el código se ejecuta múltiples veces o si las columnas ya fueron eliminadas.\n",
    "    *   Se utiliza `df.drop(columns=existing_columns_to_drop, inplace=True)` para eliminar las columnas.\n",
    "        *   `columns=...`: Especifica las columnas a eliminar.\n",
    "        *   `inplace=True`: Modifica el DataFrame `df` directamente, sin necesidad de reasignarlo (ej. `df = df.drop(...)`).\n",
    "\n",
    "3.  **Verificación:**\n",
    "    *   Mostramos `df.head()` y `df.describe()` nuevamente para confirmar que las columnas han sido eliminadas y ver el estado actual del DataFrame.\n",
    "\n",
    "**Resultado:**\n",
    "El DataFrame ahora es más limpio y contiene solo columnas potencialmente relevantes para nuestro análisis y modelado. El resumen estadístico (`df.describe()`) ahora reflejará solo estas columnas restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1d90d-efcb-4f5b-b269-990c03f83871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificación y Visualización de Valores Nulos\n",
    "\n",
    "print(\"Conteo de valores NaN por columna:\")\n",
    "nan_counts = df.isnull().sum()\n",
    "print(nan_counts[nan_counts > 0]) # Mostrar solo columnas con NaNs\n",
    "\n",
    "# Visualización de valores nulos (opcional, pero útil para datasets grandes)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Mapa de Calor de Valores Nulos en el DataFrame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178e9b5-286a-458f-ac99-a95c7e863094",
   "metadata": {},
   "source": [
    "### Identificación y Visualización de Valores Nulos - Explicación\n",
    "\n",
    "Antes de imputar (rellenar) los valores faltantes, es crucial entender dónde se encuentran y cuántos hay.\n",
    "\n",
    "1.  **Conteo de NaNs:**\n",
    "    *   `df.isnull().sum()`: Este comando realiza dos operaciones:\n",
    "        *   `df.isnull()`: Devuelve un DataFrame de las mismas dimensiones que `df`, pero con valores booleanos (`True` si el valor es NaN, `False` si no lo es).\n",
    "        *   `.sum()`: Suma estos booleanos por columna (tratando `True` como 1 y `False` como 0). El resultado es una Serie que muestra el total de NaNs para cada columna.\n",
    "    *   Filtramos para mostrar solo las columnas que efectivamente tienen valores faltantes (`nan_counts[nan_counts > 0]`).\n",
    "\n",
    "2.  **Visualización con Mapa de Calor (Opcional pero Recomendado):**\n",
    "    *   `sns.heatmap(df.isnull(), cbar=False, cmap='viridis')`: Genera un mapa de calor.\n",
    "        *   `df.isnull()`: Es la matriz booleana de NaNs.\n",
    "        *   `cbar=False`: Oculta la barra de color, ya que solo tenemos dos estados (NaN o no NaN).\n",
    "        *   `cmap='viridis'`: Especifica el esquema de color. Los NaNs se mostrarán en un color (ej. amarillo) y los no-NaNs en otro (ej. morado oscuro).\n",
    "    *   Esta visualización es muy útil para identificar patrones en los datos faltantes. Por ejemplo, si varias columnas tienen NaNs en las mismas filas, podría indicar un problema sistemático en la recolección de datos para esas observaciones.\n",
    "\n",
    "**Observaciones (basadas en el notebook original):**\n",
    "*   Las columnas `bedrooms` y `bathrooms` son las que presentan valores faltantes. Esto coincide con lo que se vio en `df.info()` y en el notebook original.\n",
    "*   El mapa de calor nos daría una representación visual de dónde se encuentran estos NaNs. Para este dataset, con solo dos columnas con pocos NaNs, puede no ser tan impactante, pero es una excelente práctica.\n",
    "\n",
    "El siguiente paso será decidir cómo tratar estos valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6403d9-173c-489b-addf-80c7b10b93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación de Valores Nulos\n",
    "\n",
    "# Detectar automáticamente columnas con NaNs\n",
    "cols_with_nans = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "print(f\"Columnas con valores nulos: {cols_with_nans}\")\n",
    "\n",
    "for col in cols_with_nans:\n",
    "    mean_val = df[col].mean()\n",
    "    df[col].fillna(mean_val, inplace=True)\n",
    "    print(f\"Valores NaN en '{col}' reemplazados con la media: {mean_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a216e3-607e-4569-bba0-fdd8717e11ff",
   "metadata": {},
   "source": [
    "### Imputación de Valores Nulos - Explicación\n",
    "\n",
    "Una vez identificados los valores faltantes, debemos decidir cómo manejarlos. Eliminar filas con NaNs es una opción, pero puede llevar a la pérdida de información valiosa, especialmente si los NaNs son pocos. Otra estrategia común es la imputación, que consiste en rellenar los NaNs con un valor estimado.\n",
    "\n",
    "1.  **Estrategia de Imputación Elegida: Media**\n",
    "    *   Para las columnas `bedrooms` y `bathrooms`, que son numéricas (aunque representan conteos, la media puede ser una aproximación razonable si la distribución no es muy sesgada y los NaNs son pocos), se ha decidido imputar los valores faltantes con la media de cada columna respectiva.\n",
    "\n",
    "2.  **Proceso de Imputación:**\n",
    "    *   Se itera sobre las columnas `['bedrooms', 'bathrooms']`.\n",
    "    *   `df[col].isnull().any()`: Se verifica si la columna actual realmente tiene algún NaN.\n",
    "    *   `mean_val = df[col].mean()`: Se calcula la media de los valores no nulos de la columna.\n",
    "    *   `df[col].fillna(mean_val, inplace=True)`: Se rellenan los NaNs (`NaN`) en la columna `col` con el valor `mean_val`.\n",
    "        *   `inplace=True` modifica el DataFrame `df` directamente.\n",
    "\n",
    "**Alternativas y Consideraciones:**\n",
    "*   **Media:** Sensible a outliers. Adecuada para distribuciones aproximadamente simétricas.\n",
    "*   **Mediana:** Más robusta a outliers. Buena opción para distribuciones sesgadas. Para conteos como `bedrooms` y `bathrooms`, la mediana podría ser incluso preferible, ya que siempre será un valor que existe en los datos (o el promedio de dos valores existentes) y es menos probable que resulte en un número fraccionario que no tenga sentido práctico (aunque la media también podría serlo).\n",
    "*   **Moda:** Adecuada para variables categóricas, pero también puede usarse para numéricas discretas.\n",
    "*   **Imputación Basada en Modelos:** Métodos más avanzados como la regresión para predecir los valores faltantes.\n",
    "*   **Eliminación:** Si el porcentaje de NaNs es muy alto en una columna, podría considerarse eliminar la columna. Si es muy bajo en filas, eliminar las filas.\n",
    "\n",
    "Para este caso, dado que la cantidad de NaNs es pequeña (13 en `bedrooms` y 10 en `bathrooms` sobre ~21600 filas), la imputación con la media (o mediana) es una solución simple y generalmente aceptable que no distorsionará significativamente los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ed2f1-0bdc-4755-8261-84acbbdde83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación Post-Imputación\n",
    "\n",
    "print(\"Conteo de valores NaN por columna después de la imputación:\")\n",
    "nan_counts_after = df.isnull().sum()\n",
    "print(nan_counts_after[nan_counts_after > 0])\n",
    "\n",
    "if nan_counts_after.sum() == 0:\n",
    "    print(\"\\n¡Excelente! No quedan valores NaN en el DataFrame.\")\n",
    "else:\n",
    "    print(\"\\nAtención: Todavía hay valores NaN en el DataFrame.\")\n",
    "\n",
    "# Ver resumen estadístico solo de columnas numéricas con imputación (como 'total_bedrooms')\n",
    "cols_to_check = ['total_bedrooms']  # Puedes agregar más si imputas otras\n",
    "\n",
    "print(\"\\nResumen estadístico de columnas imputadas después de la imputación:\")\n",
    "display(df[cols_to_check].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbc46d-017b-4bb0-8a2d-83cab70d1f90",
   "metadata": {},
   "source": [
    "### Verificación Post-Imputación - Explicación\n",
    "\n",
    "Después de aplicar cualquier técnica de limpieza o transformación de datos, es crucial verificar que los cambios se hayan realizado correctamente.\n",
    "\n",
    "1.  **Re-conteo de NaNs:**\n",
    "    *   Volvemos a ejecutar `df.isnull().sum()` para obtener el conteo actualizado de NaNs por columna.\n",
    "    *   Idealmente, las columnas `bedrooms` y `bathrooms` ahora deberían tener 0 NaNs.\n",
    "\n",
    "2.  **Confirmación General:**\n",
    "    *   `nan_counts_after.sum() == 0`: Sumamos todos los conteos de NaNs. Si el total es 0, significa que no quedan valores faltantes en todo el DataFrame.\n",
    "\n",
    "3.  **Inspección de Columnas Imputadas:**\n",
    "    *   `df[['bedrooms', 'bathrooms']].describe()`: Mostramos el resumen estadístico específicamente para las columnas que fueron imputadas. Esto nos permite observar si la media y la desviación estándar han cambiado sutilmente y si los conteos (`count`) ahora coinciden con el total de filas del DataFrame.\n",
    "\n",
    "**Resultado:**\n",
    "Esta celda confirma que el proceso de imputación ha sido exitoso y que las columnas `bedrooms` y `bathrooms` ya no contienen valores faltantes. El DataFrame está ahora un paso más cerca de estar listo para análisis más profundos y modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e8dee-5935-4d8a-a0f9-e0679fb1c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de la Característica 'ocean_proximity'\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Contar las categorías de proximidad al océano\n",
    "ocean_counts = df['ocean_proximity'].value_counts().reset_index()\n",
    "ocean_counts.columns = ['ocean_proximity', 'count']\n",
    "ocean_counts = ocean_counts.sort_values(by='ocean_proximity')\n",
    "\n",
    "print(\"Distribución de la proximidad al océano:\")\n",
    "display(ocean_counts)\n",
    "\n",
    "# Gráfico interactivo\n",
    "fig_ocean = px.bar(ocean_counts, \n",
    "                   x='ocean_proximity', \n",
    "                   y='count',\n",
    "                   title='Distribución de Proximidad al Océano',\n",
    "                   labels={'ocean_proximity': 'Categoría', 'count': 'Cantidad de viviendas'},\n",
    "                   text='count')\n",
    "fig_ocean.update_layout(xaxis_type='category')\n",
    "fig_ocean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71210ace-60e6-40fa-a70e-65b8149f3271",
   "metadata": {},
   "source": [
    "### Análisis de la Característica 'floors' - Explicación\n",
    "\n",
    "Esta celda se enfoca en entender la distribución de la característica `floors` (número de pisos de las viviendas), que es una variable numérica discreta (o podría considerarse categórica ordinal).\n",
    "\n",
    "1.  **Conteo de Valores Únicos:**\n",
    "    *   `df['floors'].value_counts()`: Calcula la frecuencia de cada valor único en la columna `floors`.\n",
    "    *   `.reset_index()`: Convierte la Serie resultante en un DataFrame, con los valores únicos como una columna y sus conteos como otra.\n",
    "    *   `floor_counts.columns = ['floors', 'count']`: Renombra las columnas para mayor claridad, especialmente útil para Plotly.\n",
    "    *   `floor_counts.sort_values(by='floors')`: Ordena los resultados por el número de pisos, lo que hace que el gráfico sea más fácil de interpretar.\n",
    "\n",
    "2.  **Visualización con Gráfico de Barras Interactivo (Plotly):**\n",
    "    *   `px.bar()`: Se utiliza la función `bar` de Plotly Express para crear un gráfico de barras.\n",
    "        *   `x='floors'`: El número de pisos en el eje X.\n",
    "        *   `y='count'`: La cantidad de viviendas en el eje Y.\n",
    "        *   `title`, `labels`: Para mejorar la legibilidad del gráfico.\n",
    "        *   `text='count'`: Muestra el valor del conteo directamente sobre cada barra.\n",
    "        *   `fig_floors.update_layout(xaxis_type='category')`: Asegura que los valores de 'floors' (ej. 1.0, 1.5, 2.0) se traten como categorías distintas en el eje X, en lugar de un continuo numérico, lo cual es más apropiado para esta variable.\n",
    "\n",
    "**Observaciones:**\n",
    "*   La tabla de `floor_counts` muestra cuántas viviendas tienen 1 piso, 1.5 pisos, 2 pisos, etc.\n",
    "*   El gráfico de barras interactivo permite visualizar esta distribución de forma clara. Se puede pasar el cursor sobre las barras para ver los conteos exactos.\n",
    "*   La mayoría de las viviendas en este dataset tienen 1 o 2 pisos. Un número significativo tiene 1.5 pisos, y hay menos viviendas con 2.5, 3 o 3.5 pisos.\n",
    "\n",
    "Este tipo de análisis es útil para entender las características predominantes en el mercado inmobiliario del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd198d-1db7-4c16-962c-5bd71278afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 13: Relación entre 'ocean_proximity' y 'median_house_value'\n",
    "\n",
    "# Crear el boxplot interactivo comparando precios según proximidad al océano\n",
    "fig_ocean_price = px.box(df, \n",
    "                         x='ocean_proximity', \n",
    "                         y='median_house_value',\n",
    "                         color='ocean_proximity',\n",
    "                         title='Distribución de Precios según Proximidad al Océano',\n",
    "                         labels={'ocean_proximity': 'Proximidad al Océano', 'median_house_value': 'Valor Mediano de la Vivienda'})\n",
    "\n",
    "fig_ocean_price.update_layout(xaxis_type='category')\n",
    "fig_ocean_price.show()\n",
    "\n",
    "# Estadísticas descriptivas agrupadas\n",
    "print(\"\\nEstadísticas de precios agrupadas por proximidad al océano:\")\n",
    "display(df.groupby('ocean_proximity')['median_house_value'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04c123-8d40-48f6-956d-7f1e62184835",
   "metadata": {},
   "source": [
    "### Relación entre 'waterfront' y 'price' - Explicación\n",
    "\n",
    "Esta celda investiga si tener una vista al mar (`waterfront`) tiene un impacto en el precio (`price`) de las viviendas. La característica `waterfront` es binaria (0 para no, 1 para sí). Un boxplot es una excelente manera de comparar las distribuciones de una variable numérica (`price`) a través de diferentes categorías de otra variable (`waterfront`).\n",
    "\n",
    "1.  **Creación del Boxplot Interactivo (Plotly):**\n",
    "    *   `px.box()`: Se utiliza la función `box` de Plotly Express.\n",
    "        *   `df`: El DataFrame que contiene los datos.\n",
    "        *   `x='waterfront'`: La variable categórica para el eje X.\n",
    "        *   `y='price'`: La variable numérica cuya distribución se quiere analizar en el eje Y.\n",
    "        *   `color='waterfront'`: Colorea las cajas de forma diferente para cada categoría de `waterfront`, mejorando la distinción visual.\n",
    "        *   `title`, `labels`: Para una mejor interpretación.\n",
    "        *   `notched=True` (Opcional): Añade \"muescas\" a las cajas. Si las muescas de dos cajas no se solapan, es una indicación (aunque no una prueba formal) de que las medianas son significativamente diferentes.\n",
    "    *   `fig_waterfront_price.update_layout(xaxis_type='category')`: Se asegura que `waterfront` (0 y 1) se trate como categorías.\n",
    "\n",
    "2.  **Estadísticas Descriptivas Agrupadas (Opcional):**\n",
    "    *   `df.groupby('waterfront')['price'].describe()`: Calcula estadísticas descriptivas (media, mediana, desviación estándar, cuartiles, etc.) para la columna `price`, agrupadas por los valores de `waterfront`. Esto complementa la visualización con números concretos.\n",
    "\n",
    "**Observaciones:**\n",
    "*   **Visualización:** El boxplot mostrará dos cajas, una para las casas sin vista al mar (0) y otra para las que sí la tienen (1).\n",
    "    *   La **línea media** de cada caja representa la mediana del precio.\n",
    "    *   La **altura de la caja** representa el rango intercuartílico (IQR, la diferencia entre el percentil 75 y el 25).\n",
    "    *   Los **\"bigotes\"** se extienden típicamente hasta 1.5 veces el IQR desde los bordes de la caja.\n",
    "    *   Los **puntos fuera de los bigotes** son considerados outliers.\n",
    "*   **Interpretación Esperada:** Es muy probable que las casas con vista al mar (`waterfront` = 1) tengan precios medianos y generales significativamente más altos que aquellas sin vista al mar. El boxplot permitirá comparar la dispersión de los precios también.\n",
    "*   **Interacción:** Al usar Plotly, se puede pasar el cursor sobre las cajas para ver los valores exactos de los cuartiles, la mediana, y los outliers.\n",
    "\n",
    "Este análisis ayuda a confirmar la intuición de que características premium como la vista al mar tienen una influencia positiva y considerable en el precio de una vivienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3ac6f-8445-42f2-8ba1-0aee18f52032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre 'median_income' y 'median_house_value'\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Crear scatter plot con línea de tendencia\n",
    "fig_income_price = px.scatter(df, \n",
    "                              x='median_income', \n",
    "                              y='median_house_value',\n",
    "                              title='Relación entre Ingreso Medio y Valor Mediano de la Vivienda',\n",
    "                              labels={'median_income': 'Ingreso Medio', 'median_house_value': 'Valor Mediano de la Vivienda'},\n",
    "                              opacity=0.5,\n",
    "                              trendline='ols')  # Línea de regresión\n",
    "\n",
    "fig_income_price.show()\n",
    "\n",
    "# Correlación\n",
    "correlation = df['median_income'].corr(df['median_house_value'])\n",
    "print(f\"\\nCoeficiente de correlación de Pearson entre 'median_income' y 'median_house_value': {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d651a6d-5e0a-4cfd-aa2b-a7933e9c984b",
   "metadata": {},
   "source": [
    "### Relación entre 'sqft_above' y 'price' - Explicación\n",
    "\n",
    "Esta celda explora la relación entre `sqft_above` (pies cuadrados de la vivienda que están por encima del nivel del suelo) y `price` (el precio de la vivienda). Ambas son variables numéricas continuas, por lo que un diagrama de dispersión (scatter plot) es apropiado.\n",
    "\n",
    "1.  **Creación del Scatter Plot Interactivo con Línea de Tendencia (Plotly):**\n",
    "    *   `px.scatter()`: Se utiliza la función `scatter` de Plotly Express.\n",
    "        *   `df`: El DataFrame.\n",
    "        *   `x='sqft_above'`: Variable para el eje X.\n",
    "        *   `y='price'`: Variable para el eje Y.\n",
    "        *   `title`, `labels`: Para la legibilidad.\n",
    "        *   `opacity=0.5`: Hace los puntos semi-transparentes. Esto es útil cuando hay muchos puntos superpuestos, ya que ayuda a visualizar la densidad.\n",
    "        *   `trendline='ols'`: Añade automáticamente una línea de regresión lineal ajustada usando el método de Mínimos Cuadrados Ordinarios (Ordinary Least Squares). Esta línea muestra la tendencia general en la relación entre las dos variables.\n",
    "\n",
    "2.  **Cálculo del Coeficiente de Correlación:**\n",
    "    *   `df['sqft_above'].corr(df['price'])`: Calcula el coeficiente de correlación de Pearson entre `sqft_above` y `price`. Este coeficiente mide la fuerza y dirección de la relación lineal entre dos variables.\n",
    "        *   Varía entre -1 y +1.\n",
    "        *   Un valor cercano a +1 indica una fuerte correlación lineal positiva.\n",
    "        *   Un valor cercano a -1 indica una fuerte correlación lineal negativa.\n",
    "        *   Un valor cercano a 0 indica una débil o nula correlación lineal.\n",
    "\n",
    "**Observaciones e Interpretación Esperada:**\n",
    "*   **Visualización:** El scatter plot mostrará una nube de puntos. Cada punto representa una vivienda.\n",
    "    *   Se espera una tendencia general ascendente: a medida que `sqft_above` aumenta, `price` también tiende a aumentar.\n",
    "    *   La línea de tendencia OLS visualizará esta relación lineal promedio.\n",
    "    *   La dispersión de los puntos alrededor de la línea de tendencia indica la variabilidad. Es probable que haya una considerable dispersión, ya que el precio depende de muchos otros factores además de `sqft_above`.\n",
    "*   **Correlación:** Se espera un coeficiente de correlación positivo y moderadamente fuerte (por ejemplo, entre 0.4 y 0.7).\n",
    "*   **Interacción:** Con Plotly, se puede hacer zoom, panear y pasar el cursor sobre los puntos para ver sus valores exactos.\n",
    "\n",
    "Este análisis es un paso fundamental para entender cómo las características individuales se relacionan con la variable objetivo (precio) y para la selección de características en el modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1a798-0066-4471-af13-ad5119e71de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Matriz de Correlación (para variables numéricas)\n",
    "correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Matriz de Correlación de Características Numéricas')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelaciones más altas con 'median_house_value':\")\n",
    "display(correlation_matrix['median_house_value'].sort_values(ascending=False).head(10))\n",
    "\n",
    "# 2. Feature Engineering básico\n",
    "# Crear variable categórica simple: alta densidad poblacional\n",
    "df['high_population_density'] = (df['population'] / df['households']) > 3  # por ejemplo\n",
    "\n",
    "# Crear una nueva feature: habitaciones por persona\n",
    "df['rooms_per_person'] = df['total_rooms'] / df['population']\n",
    "df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "\n",
    "# Visualizar relaciones con el valor de la vivienda\n",
    "fig1 = px.scatter(df, x='rooms_per_person', y='median_house_value',\n",
    "                  title='Relación entre Habitaciones por Persona y Valor de la Vivienda',\n",
    "                  opacity=0.5, trendline='ols')\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.scatter(df, x='bedrooms_per_room', y='median_house_value',\n",
    "                  title='Relación entre Proporción de Dormitorios por Habitación y Valor de la Vivienda',\n",
    "                  opacity=0.5, trendline='ols')\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7861b33-8408-4adc-be90-bb374ad68bea",
   "metadata": {},
   "source": [
    "### Matriz de Correlación y Feature Engineering Básico - Explicación\n",
    "\n",
    "Esta celda expande nuestro Análisis Exploratorio de Datos (EDA) con dos técnicas importantes: el análisis de correlación y la ingeniería de características.\n",
    "\n",
    "1.  **Matriz de Correlación:**\n",
    "    *   `df.corr(numeric_only=True)`: Calcula la correlación de Pearson entre todas las pares de columnas numéricas del DataFrame.\n",
    "    *   `sns.heatmap(...)`: Visualiza esta matriz de correlación como un mapa de calor.\n",
    "        *   `annot=True`: Muestra los valores de correlación en cada celda.\n",
    "        *   `cmap='coolwarm'`: Esquema de color donde los colores cálidos (rojos) indican correlación positiva y los fríos (azules) indican correlación negativa.\n",
    "        *   `fmt=\".2f\"`: Formatea los números a dos decimales.\n",
    "    *   También imprimimos las 10 características con mayor correlación (positiva o negativa, en valor absoluto) con `price`.\n",
    "\n",
    "    **Interpretación de la Matriz de Correlación:**\n",
    "    *   Nos ayuda a identificar multicolinealidad (alta correlación entre variables predictoras), lo cual puede ser un problema para algunos modelos.\n",
    "    *   Muestra qué variables están más fuertemente relacionadas (linealmente) con la variable objetivo (`price`). Características como `sqft_living`, `grade`, y `sqft_above` suelen tener altas correlaciones positivas con el precio.\n",
    "\n",
    "2.  **Feature Engineering (Ingeniería de Características):**\n",
    "    La ingeniería de características es el proceso de crear nuevas variables (features) a partir de las existentes para mejorar el rendimiento del modelo.\n",
    "    *   **Conversión de `date`:** La columna `date` (fecha de venta) se convierte al formato `datetime` para poder extraer componentes temporales.\n",
    "    *   **`sale_year`:** Se extrae el año de la venta.\n",
    "    *   **`age_at_sale`:** Se calcula la edad de la casa en el momento de la venta (`sale_year - yr_built`). Intuitivamente, las casas más antiguas podrían tener precios diferentes.\n",
    "    *   **`was_renovated`:** Una variable binaria (0 o 1) que indica si la casa ha sido renovada (`yr_renovated != 0`).\n",
    "    *   **`age_since_renovation_or_build`:** Calcula la edad de la casa desde su última renovación o, si no fue renovada, desde su construcción. Esto podría ser más relevante que la simple edad de construcción.\n",
    "    *   Se crea un nuevo DataFrame `df` eliminando las columnas originales de fecha/año que ya no se usarán directamente, para evitar redundancia y posible confusión para el modelo.\n",
    "    *   Se muestran las primeras filas de estas nuevas características junto con el precio.\n",
    "    *   Se generan diagramas de dispersión interactivos para visualizar la relación entre estas nuevas características de \"edad\" y el precio.\n",
    "\n",
    "**Beneficios:**\n",
    "*   La **matriz de correlación** nos guía en la selección de características y en la comprensión de las interrelaciones en los datos.\n",
    "*   La **ingeniería de características** puede capturar información más matizada. Por ejemplo, una casa antigua pero recientemente renovada podría tener un valor similar o superior a una casa más nueva pero no renovada. `age_since_renovation_or_build` intenta capturar esta idea.\n",
    "\n",
    "Estos pasos enriquecen nuestro conjunto de datos y nos preparan mejor para la fase de modelado. Las nuevas características generadas se incluirán en los modelos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fa320-a6cf-4a04-8e66-6bcad2075f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Definimos X y Y con columnas existentes\n",
    "X_simple = df[['total_rooms']]\n",
    "Y_simple = df['median_house_value']\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "lm_simple = LinearRegression()\n",
    "\n",
    "# Ajustar el modelo a los datos\n",
    "lm_simple.fit(X_simple, Y_simple)\n",
    "\n",
    "# Realizar predicciones\n",
    "Y_pred_simple = lm_simple.predict(X_simple)\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "r2_simple = r2_score(Y_simple, Y_pred_simple)\n",
    "mse_simple = mean_squared_error(Y_simple, Y_pred_simple)\n",
    "mae_simple = mean_absolute_error(Y_simple, Y_pred_simple)\n",
    "rmse_simple = np.sqrt(mse_simple)\n",
    "\n",
    "# Imprimir coeficientes y métricas\n",
    "print(f\"Regresión Lineal Simple: 'total_rooms' vs 'median_house_value'\")\n",
    "print(f\"Intercepto (b0): {lm_simple.intercept_:.2f}\")\n",
    "print(f\"Coeficiente para 'total_rooms' (b1): {lm_simple.coef_[0]:.2f}\")\n",
    "print(f\"R² (Coeficiente de Determinación): {r2_simple:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_simple:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_simple:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_simple:.2f}\")\n",
    "\n",
    "# Visualización con matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, Y_simple, color='blue', alpha=0.3, label='Datos Reales')\n",
    "plt.plot(X_simple, Y_pred_simple, color='red', linewidth=2, label='Línea de Regresión')\n",
    "plt.title('Regresión Lineal Simple: total_rooms vs median_house_value')\n",
    "plt.xlabel('Total Rooms')\n",
    "plt.ylabel('Valor Medio de la Vivienda')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualización con Plotly (interactivo)\n",
    "fig_simple_plotly = px.scatter(x=X_simple['total_rooms'], y=Y_simple, opacity=0.3,\n",
    "                               labels={'x': 'Total Rooms', 'y': 'Valor Medio de la Vivienda'},\n",
    "                               title='Regresión Lineal Simple Interactiva (Plotly)')\n",
    "fig_simple_plotly.add_traces(go.Scatter(x=X_simple['total_rooms'], y=Y_pred_simple, name='Línea de Regresión', line=dict(color='red')))\n",
    "fig_simple_plotly.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df836949-5741-4bd6-b182-14097c5186c8",
   "metadata": {},
   "source": [
    "### Regresión Lineal Simple (`sqft_living` vs `price`) - Explicación\n",
    "\n",
    "Esta celda construye y evalúa un modelo de regresión lineal simple. Este tipo de modelo busca establecer una relación lineal entre una única variable independiente (predictora) y una variable dependiente (objetivo).\n",
    "\n",
    "**Modelo:** `price = b0 + b1 * sqft_living`\n",
    "\n",
    "1.  **Selección de Variables:**\n",
    "    *   Variable Independiente (X): `sqft_living` (los pies cuadrados de la vivienda).\n",
    "    *   Variable Dependiente (Y): `price` (el precio de la vivienda).\n",
    "    *   Se utiliza `df` que ya incluye las características de edad generadas previamente, aunque para este modelo particular solo nos centramos en `sqft_living`.\n",
    "\n",
    "2.  **Creación y Ajuste del Modelo:**\n",
    "    *   `lm_simple = LinearRegression()`: Se instancia un objeto del modelo de Regresión Lineal de `scikit-learn`.\n",
    "    *   `lm_simple.fit(X_simple, Y_simple)`: Se ajusta el modelo a los datos. El modelo \"aprende\" los valores óptimos para el intercepto (`b0`) y el coeficiente (`b1`) que mejor describen la relación lineal.\n",
    "\n",
    "3.  **Predicciones:**\n",
    "    *   `Y_pred_simple = lm_simple.predict(X_simple)`: Se utilizan los datos de `sqft_living` para predecir los precios.\n",
    "\n",
    "4.  **Evaluación del Modelo:**\n",
    "    *   **Intercepto (`lm_simple.intercept_`):** El valor de `price` cuando `sqft_living` es 0. En este contexto, puede no tener una interpretación práctica directa, pero es parte de la ecuación lineal.\n",
    "    *   **Coeficiente (`lm_simple.coef_`):** Indica cuánto cambia `price` (en promedio) por cada unidad de aumento en `sqft_living`. Un coeficiente positivo significa que a mayor `sqft_living`, mayor `price`.\n",
    "    *   **R² (Coeficiente de Determinación):** `r2_score(Y_simple, Y_pred_simple)`. Mide la proporción de la varianza en la variable dependiente (`price`) que es predecible a partir de la variable independiente (`sqft_living`). Varía entre 0 y 1 (o 0% y 100%). Un valor más alto indica un mejor ajuste del modelo. El R² original era ~0.49.\n",
    "    *   **Mean Squared Error (MSE):** `mean_squared_error(Y_simple, Y_pred_simple)`. Es el promedio de los errores al cuadrado. Penaliza más los errores grandes.\n",
    "    *   **Root Mean Squared Error (RMSE):** `np.sqrt(mse_simple)`. Es la raíz cuadrada del MSE. Tiene las mismas unidades que la variable objetivo (`price`), lo que facilita su interpretación como una medida típica del error de predicción.\n",
    "    *   **Mean Absolute Error (MAE):** `mean_absolute_error(Y_simple, Y_pred_simple)`. Es el promedio de los valores absolutos de los errores. También está en las unidades de la variable objetivo.\n",
    "\n",
    "5.  **Visualización:**\n",
    "    *   Se genera un diagrama de dispersión de los datos reales (`sqft_living` vs `price`).\n",
    "    *   Sobre este diagrama, se superpone la línea de regresión (`sqft_living` vs `Y_pred_simple`) aprendida por el modelo.\n",
    "    *   Se incluye una versión interactiva con Plotly, que permite explorar los datos y la línea de regresión más de cerca.\n",
    "\n",
    "**Interpretación:**\n",
    "*   El R² nos dirá qué porcentaje de la variación en los precios de las casas se explica por los pies cuadrados de la vivienda.\n",
    "*   El coeficiente de `sqft_living` nos indicará el impacto monetario estimado de cada pie cuadrado adicional.\n",
    "*   El RMSE y MAE nos darán una idea de cuán lejos, en promedio, están las predicciones del modelo de los precios reales.\n",
    "*   La visualización nos mostrará si la relación lineal es una buena aproximación para estos datos.\n",
    "\n",
    "Este modelo simple sirve como una línea base para comparar con modelos más complejos que incluyan múltiples características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbefcb3-24ca-4cd5-989a-6d1a75f25cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Selección de variables (basado en el dataset California Housing)\n",
    "features_multiple = [\n",
    "    \"housing_median_age\", \"total_rooms\", \"total_bedrooms\",\n",
    "    \"population\", \"households\", \"median_income\"\n",
    "]\n",
    "\n",
    "X_multiple = df[features_multiple]\n",
    "Y_multiple = df[\"median_house_value\"]\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "lm_multiple = LinearRegression()\n",
    "lm_multiple.fit(X_multiple, Y_multiple)\n",
    "\n",
    "# Predicciones\n",
    "Y_pred_multiple = lm_multiple.predict(X_multiple)\n",
    "\n",
    "# Métricas\n",
    "r2_multiple = r2_score(Y_multiple, Y_pred_multiple)\n",
    "mse_multiple = mean_squared_error(Y_multiple, Y_pred_multiple)\n",
    "mae_multiple = mean_absolute_error(Y_multiple, Y_pred_multiple)\n",
    "rmse_multiple = np.sqrt(mse_multiple)\n",
    "\n",
    "print(f\"Regresión Lineal Múltiple con {len(features_multiple)} características\")\n",
    "print(f\"Intercepto (b0): {lm_multiple.intercept_:.2f}\")\n",
    "print(f\"R² (Coeficiente de Determinación): {r2_multiple:.4f}\")\n",
    "print(f\"MSE: {mse_multiple:.2f}\")\n",
    "print(f\"RMSE: {rmse_multiple:.2f}\")\n",
    "print(f\"MAE: {mae_multiple:.2f}\")\n",
    "\n",
    "# Coeficientes\n",
    "coeffs = pd.DataFrame(lm_multiple.coef_, features_multiple, columns=['Coeficiente'])\n",
    "display(coeffs.sort_values(by='Coeficiente', ascending=False))\n",
    "\n",
    "# Visualización: Predicho vs Real\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_multiple, Y_pred_multiple, alpha=0.3, color='purple')\n",
    "plt.plot([Y_multiple.min(), Y_multiple.max()], [Y_multiple.min(), Y_multiple.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valor Real de la Vivienda')\n",
    "plt.ylabel('Valor Predicho')\n",
    "plt.title('Regresión Múltiple: Valor Real vs. Valor Predicho')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fe674-c8c2-4bb0-99d0-9281cdd36a90",
   "metadata": {},
   "source": [
    "### Regresión Lineal Múltiple - Explicación\n",
    "\n",
    "Ampliamos nuestro modelo para incluir múltiples variables independientes (características) con el objetivo de mejorar la predicción del precio (`price`). Este enfoque se conoce como Regresión Lineal Múltiple.\n",
    "\n",
    "**Modelo:** `price = b0 + b1*feature1 + b2*feature2 + ... + bn*featureN`\n",
    "\n",
    "1.  **Selección de Características:**\n",
    "    *   Se define una lista `features_multiple` que incluye las características originales relevantes y las nuevas que hemos creado mediante ingeniería de características (`age_at_sale`, `was_renovated`, `age_since_renovation_or_build`).\n",
    "    *   Variable Independientes (X): El subconjunto del DataFrame `df` con estas características.\n",
    "    *   Variable Dependiente (Y): `price`.\n",
    "\n",
    "2.  **Creación y Ajuste del Modelo:**\n",
    "    *   `lm_multiple = LinearRegression()`: Se instancia el modelo.\n",
    "    *   `lm_multiple.fit(X_multiple, Y_multiple)`: Se ajusta el modelo, aprendiendo el intercepto `b0` y un coeficiente `bi` para cada característica.\n",
    "\n",
    "3.  **Predicciones y Evaluación:**\n",
    "    *   Se realizan predicciones (`Y_pred_multiple`).\n",
    "    *   Se calculan las mismas métricas que en la regresión simple: R², MSE, RMSE, MAE.\n",
    "    *   **Intercepto (`lm_multiple.intercept_`):** El valor predicho de `price` cuando todas las características son 0.\n",
    "    *   **Coeficientes (`lm_multiple.coef_`):** Se muestra un DataFrame con los coeficientes para cada característica. Cada coeficiente `bi` representa el cambio esperado en `price` por un aumento de una unidad en `feature_i`, *manteniendo todas las demás características constantes*.\n",
    "        *   **Importante:** La magnitud de los coeficientes no indica directamente la importancia de la característica si las variables no están en la misma escala. Para una mejor interpretación de la importancia, las características deberían ser escaladas (normalizadas/estandarizadas) antes de ajustar el modelo. Sin embargo, el signo (positivo/negativo) sigue siendo informativo.\n",
    "\n",
    "4.  **Visualización (Predicciones vs. Reales):**\n",
    "    *   Se crea un diagrama de dispersión que compara los precios reales (`Y_multiple`) con los precios predecichos por el modelo (`Y_pred_multiple`).\n",
    "    *   Se añade una línea diagonal (`y=x`). Si el modelo fuera perfecto, todos los puntos caerían sobre esta línea. La dispersión alrededor de esta línea da una idea visual de la precisión del modelo.\n",
    "\n",
    "**Interpretación Esperada:**\n",
    "*   **R²:** Se espera que el R² sea significativamente mayor que en el modelo de regresión simple, ya que más información (características) se está utilizando para predecir el precio. El R² original con las features dadas era ~0.65. Con las nuevas features de edad, podría mejorar ligeramente.\n",
    "*   **RMSE/MAE:** Se espera que estos errores sean menores que en el modelo simple.\n",
    "*   **Coeficientes:**\n",
    "    *   `sqft_living`, `grade`, `bathrooms`, `waterfront` (si es 1) probablemente tendrán coeficientes positivos.\n",
    "    *   `age_at_sale` o `age_since_renovation_or_build` podrían tener coeficientes negativos (casas más viejas tienden a ser más baratas, si todo lo demás es igual).\n",
    "    *   `lat` (latitud) podría tener un coeficiente positivo o negativo grande dependiendo de la geografía y cómo se correlaciona con áreas de alto/bajo valor.\n",
    "*   **Gráfico Predicciones vs. Reales:** Los puntos deberían agruparse más cerca de la línea diagonal que en un modelo menos preciso.\n",
    "\n",
    "Este modelo múltiple proporciona una comprensión más rica de los factores que influyen en el precio de la vivienda. Sin embargo, es importante recordar que estas predicciones y métricas se calculan sobre los mismos datos utilizados para entrenar el modelo, lo que puede llevar a una sobreestimación del rendimiento en datos no vistos. Más adelante, utilizaremos técnicas como la división en conjuntos de entrenamiento/prueba y la validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a56f4d-1c91-4ba3-8ad0-398b87319a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Seleccionar variables relevantes\n",
    "features_poly = [\n",
    "    \"housing_median_age\", \"total_rooms\", \"total_bedrooms\",\n",
    "    \"population\", \"households\", \"median_income\"\n",
    "]\n",
    "\n",
    "X_poly = df[features_poly]\n",
    "Y_poly = df[\"median_house_value\"]\n",
    "\n",
    "# Crear el pipeline de regresión polinómica\n",
    "Input_pipeline = [\n",
    "    ('scale', StandardScaler()),  # Estandarizar\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),  # Términos polinómicos\n",
    "    ('model', LinearRegression())  # Regresión lineal\n",
    "]\n",
    "\n",
    "pipe = Pipeline(Input_pipeline)\n",
    "\n",
    "# Entrenar\n",
    "pipe.fit(X_poly, Y_poly)\n",
    "\n",
    "# Predecir\n",
    "Y_pred_pipe = pipe.predict(X_poly)\n",
    "\n",
    "# Evaluación\n",
    "r2_pipe = pipe.score(X_poly, Y_poly)\n",
    "mse_pipe = mean_squared_error(Y_poly, Y_pred_pipe)\n",
    "mae_pipe = mean_absolute_error(Y_poly, Y_pred_pipe)\n",
    "rmse_pipe = np.sqrt(mse_pipe)\n",
    "\n",
    "print(f\"Regresión Polinómica (Grado 2) con Pipeline\")\n",
    "print(f\"R²: {r2_pipe:.4f}\")\n",
    "print(f\"MSE: {mse_pipe:.2f}\")\n",
    "print(f\"RMSE: {rmse_pipe:.2f}\")\n",
    "print(f\"MAE: {mae_pipe:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862c527-331c-4c01-ba6e-39dd0e9694ad",
   "metadata": {},
   "source": [
    "### Regresión Polinómica con Pipeline - Explicación\n",
    "\n",
    "Esta celda introduce la Regresión Polinómica y el uso de `Pipeline` de `scikit-learn`. La regresión polinómica permite capturar relaciones no lineales entre las características y la variable objetivo al añadir términos polinómicos (ej. x², x³, x*y) al modelo. Un `Pipeline` ayuda a organizar y automatizar una secuencia de pasos de preprocesamiento y modelado.\n",
    "\n",
    "1.  **Selección de Características:**\n",
    "    *   Se utilizan las mismas `features_multiple` que en la celda anterior, provenientes de `df`.\n",
    "\n",
    "2.  **Creación del Pipeline (`Input_pipeline`):**\n",
    "    Un `Pipeline` encadena múltiples transformadores y un estimador final. Los datos pasan por cada paso secuencialmente.\n",
    "    *   **`('scale', StandardScaler())`**:\n",
    "        *   Este es el primer paso. `StandardScaler` estandariza las características eliminando la media y escalando a la varianza unitaria. Esto es importante para la regresión polinómica y para muchos otros algoritmas, ya que asegura que todas las características contribuyan de manera equitativa y puede mejorar la convergencia y el rendimiento.\n",
    "    *   **`('polynomial', PolynomialFeatures(degree=2, include_bias=False))`**:\n",
    "        *   Este es el segundo paso. `PolynomialFeatures` genera nuevas características que son combinaciones polinómicas de las características originales.\n",
    "            *   `degree=2`: Crea términos hasta el segundo grado (ej., si tenemos `a` y `b`, genera `a`, `b`, `a²`, `b²`, `a*b`).\n",
    "            *   `include_bias=False`: No añade una columna de unos para el intercepto, ya que `LinearRegression` lo maneja.\n",
    "        *   El número de características puede aumentar significativamente. Si teníamos N características, ahora tendremos N (originales) + N (cuadradas) + N*(N-1)/2 (interacciones).\n",
    "    *   **`('model', LinearRegression())`**:\n",
    "        *   Este es el estimador final. Es un modelo de regresión lineal, pero ahora se ajustará a las características originales *más* los términos polinómicos generados.\n",
    "\n",
    "3.  **Ajuste y Evaluación del Pipeline:**\n",
    "    *   `pipe = Pipeline(Input_pipeline)`: Se crea el objeto `Pipeline`.\n",
    "    *   `pipe.fit(X_poly, Y_poly)`: Se ajusta el pipeline completo. Los datos `X_poly` se pasan primero por `StandardScaler`, luego el resultado por `PolynomialFeatures`, y finalmente el modelo `LinearRegression` se ajusta a estas características transformadas.\n",
    "    *   `pipe.predict(X_poly)` y `pipe.score(X_poly, Y_poly)`: Para predicciones y R². `score` automáticamente aplica todas las transformaciones a `X_poly` antes de usar el modelo para calcular R².\n",
    "    *   Se calculan también MSE, RMSE y MAE.\n",
    "\n",
    "**Interpretación Esperada:**\n",
    "*   **R²:** Se espera que el R² sea mayor que el de la regresión lineal múltiple simple (el original era ~0.75). Los términos polinómicos permiten al modelo capturar relaciones más complejas.\n",
    "*   **Métricas de Error (MSE, RMSE, MAE):** Deberían disminuir en comparación con el modelo lineal múltiple.\n",
    "*   **Sobreajuste (Overfitting):** Con la regresión polinómica, especialmente con grados altos o muchas características, existe un riesgo de sobreajuste. El modelo podría ajustarse muy bien a los datos de entrenamiento pero generalizar mal a datos nuevos. El R² aquí se calcula sobre los datos de entrenamiento, por lo que podría estar inflado. El uso de conjuntos de entrenamiento/prueba y validación cruzada (que veremos a continuación) es crucial para detectar y mitigar el sobreajuste.\n",
    "*   **Interpretabilidad de Coeficientes:** Se vuelve mucho más difícil interpretar los coeficientes individuales debido a la gran cantidad de términos polinómicos y de interacción. El enfoque se desplaza más hacia la capacidad predictiva general del modelo.\n",
    "\n",
    "El uso de `Pipeline` es una práctica excelente en machine learning, ya que simplifica el flujo de trabajo, evita fugas de datos (data leakage) al aplicar transformaciones correctamente en diferentes conjuntos de datos (ej. entrenamiento vs. prueba), y facilita la reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b1deb-7c85-4658-a544-9b6cb163000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Ridge (Regularización L2)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Importante escalar para Ridge\n",
    "\n",
    "# Usar las mismas características que en la regresión múltiple (incluyendo las de edad)\n",
    "# features_multiple ya está definida y verificada\n",
    "X_ridge = df[features_multiple]\n",
    "Y_ridge = df[\"median_house_value\"]\n",
    "\n",
    "# 1. Dividir los datos en entrenamiento (80%) y prueba (20%)\n",
    "# Usamos un test_size más estándar de 0.2 o 0.25. Random_state para reproducibilidad.\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_ridge, Y_ridge, test_size=0.20, random_state=42)\n",
    "\n",
    "# 2. Escalar las características\n",
    "# Es importante escalar los datos ANTES de aplicar Ridge, ya que es sensible a la escala de las features.\n",
    "# Ajustar el escalador SOLO con los datos de entrenamiento para evitar data leakage.\n",
    "scaler_ridge = StandardScaler()\n",
    "x_train_scaled = scaler_ridge.fit_transform(x_train)\n",
    "x_test_scaled = scaler_ridge.transform(x_test) # Aplicar la misma transformación al conjunto de prueba\n",
    "\n",
    "# 3. Crear y ajustar el modelo Ridge\n",
    "# alpha es el parámetro de regularización. Valores más altos implican mayor regularización.\n",
    "ridge_model = Ridge(alpha=0.1) \n",
    "ridge_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# 4. Realizar predicciones en el conjunto de prueba\n",
    "y_pred_test_ridge = ridge_model.predict(x_test_scaled)\n",
    "\n",
    "# 5. Calcular métricas de evaluación EN EL CONJUNTO DE PRUEBA\n",
    "r2_test_ridge = ridge_model.score(x_test_scaled, y_test) # R² en datos de prueba\n",
    "mse_test_ridge = mean_squared_error(y_test, y_pred_test_ridge)\n",
    "mae_test_ridge = mean_absolute_error(y_test, y_pred_test_ridge)\n",
    "rmse_test_ridge = np.sqrt(mse_test_ridge)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"Regresión Ridge (alpha=0.1) con división entrenamiento/prueba\")\n",
    "print(f\"Número de muestras de entrenamiento: {x_train.shape[0]}\")\n",
    "print(f\"Número de muestras de prueba: {x_test.shape[0]}\")\n",
    "print(f\"\\nR² en datos de prueba: {r2_test_ridge:.4f}\")\n",
    "print(f\"MSE en datos de prueba: {mse_test_ridge:.2f}\")\n",
    "print(f\"RMSE en datos de prueba: {rmse_test_ridge:.2f}\")\n",
    "print(f\"MAE en datos de prueba: {mae_test_ridge:.2f}\")\n",
    "\n",
    "# Coeficientes del modelo Ridge\n",
    "print(\"\\nCoeficientes del modelo Ridge (después de escalar):\")\n",
    "coeffs_ridge = pd.DataFrame(ridge_model.coef_, X_ridge.columns, columns=['Coefficient'])\n",
    "display(coeffs_ridge.sort_values(by='Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ca829-39aa-488e-afc2-7fccdd9f1928",
   "metadata": {},
   "source": [
    "### Regresión Ridge (Regularización L2) - Explicación\n",
    "\n",
    "Esta celda introduce dos conceptos cruciales: la división de datos en conjuntos de entrenamiento y prueba, y la Regresión Ridge, un tipo de regresión lineal regularizada.\n",
    "\n",
    "1.  **División en Conjuntos de Entrenamiento y Prueba (`train_test_split`):**\n",
    "    *   `x_train, x_test, y_train, y_test = train_test_split(X_ridge, Y_ridge, test_size=0.20, random_state=42)`\n",
    "        *   `test_size=0.20`: Reserva el 20% de los datos para el conjunto de prueba y el 80% restante para el entrenamiento. El notebook original usaba 15%, pero 20% o 25% es más común.\n",
    "        *   `random_state=42`: Asegura que la división sea la misma cada vez que se ejecuta el código, lo que garantiza la reproducibilidad de los resultados.\n",
    "    *   **Propósito:** Entrenamos el modelo con el conjunto de entrenamiento y luego evaluamos su rendimiento en el conjunto de prueba (datos que el modelo no ha visto antes). Esto nos da una estimación más realista de cómo el modelo generalizará a nuevos datos y ayuda a detectar el sobreajuste.\n",
    "\n",
    "2.  **Escalado de Características (`StandardScaler`):**\n",
    "    *   `scaler_ridge = StandardScaler()`\n",
    "    *   `x_train_scaled = scaler_ridge.fit_transform(x_train)`: Se ajusta el escalador *solo* con los datos de entrenamiento (para aprender la media y desviación estándar de cada característica en el entrenamiento) y luego se transforman esos datos.\n",
    "    *   `x_test_scaled = scaler_ridge.transform(x_test)`: Se aplica la *misma* transformación aprendida en el conjunto de entrenamiento al conjunto de prueba. Es crucial no volver a ajustar (`fit`) el escalador con los datos de prueba para evitar la fuga de información (data leakage) del conjunto de prueba al de entrenamiento.\n",
    "    *   **Propósito:** La Regresión Ridge (y muchas otras) es sensible a la escala de las características. El escalado asegura que todas las características tengan una escala similar (media 0, desviación estándar 1), lo que permite que el término de regularización penalice los coeficientes de manera justa.\n",
    "\n",
    "3.  **Regresión Ridge (`Ridge`):**\n",
    "    *   `ridge_model = Ridge(alpha=0.1)`: La Regresión Ridge añade una penalización L2 (suma de los cuadrados de los coeficientes) a la función de coste de la regresión lineal.\n",
    "        *   `alpha`: Es el parámetro de regularización. Controla la fuerza de la penalización. Un `alpha` más alto resulta en coeficientes más pequeños (más regularización). `alpha=0` sería una regresión lineal ordinaria. El valor de 0.1 es un punto de partida común; en la práctica, se ajustaría mediante validación cruzada.\n",
    "    *   `ridge_model.fit(x_train_scaled, y_train)`: Se ajusta el modelo Ridge a los datos de entrenamiento *escalados*.\n",
    "    *   **Propósito:** La regularización ayuda a prevenir el sobreajuste, especialmente cuando hay multicolinealidad (características altamente correlacionadas) o cuando el número de características es grande. Tiende a encoger los coeficientes de las características menos importantes hacia cero.\n",
    "\n",
    "4.  **Evaluación en el Conjunto de Prueba:**\n",
    "    *   `y_pred_test_ridge = ridge_model.predict(x_test_scaled)`: Se realizan predicciones sobre el conjunto de prueba *escalado*.\n",
    "    *   Las métricas (R², MSE, RMSE, MAE) se calculan comparando `y_test` (valores reales del conjunto de prueba) con `y_pred_test_ridge` (predicciones para el conjunto de prueba). El R² original en el test era ~0.64.\n",
    "\n",
    "**Interpretación:**\n",
    "*   El **R² en el conjunto de prueba** es la métrica clave aquí. Nos dice qué tan bien el modelo generaliza a datos no vistos. Si es significativamente menor que el R² en el conjunto de entrenamiento (que obtendríamos si evaluáramos `ridge_model.score(x_train_scaled, y_train)`), podría indicar sobreajuste.\n",
    "*   Los **coeficientes** del modelo Ridge, al estar las características escaladas, pueden dar una idea más comparable de la importancia relativa de las características.\n",
    "*   La Regresión Ridge es un buen compromiso entre la simplicidad de la regresión lineal y la robustez contra el sobreajuste.\n",
    "\n",
    "Este es un flujo de trabajo mucho más robusto para la construcción y evaluación de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb3caf-d536-4b78-84e1-cef54d0979da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Ridge Polinómica con Pipeline y train_test_split\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Usar las mismas características que antes\n",
    "# features_multiple ya está definida y verificada\n",
    "X_poly_ridge = df[features_multiple]\n",
    "Y_poly_ridge = df['median_house_value']\n",
    "\n",
    "# Dividir los datos en entrenamiento (80%) y prueba (20%)\n",
    "x_train_pr, x_test_pr, y_train_pr, y_test_pr = train_test_split(X_poly_ridge, Y_poly_ridge, test_size=0.20, random_state=42)\n",
    "\n",
    "# Crear el pipeline para Regresión Ridge Polinómica\n",
    "# El orden es importante: escalar -> generar features polinómicas -> modelo Ridge\n",
    "poly_ridge_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('ridge_model', Ridge(alpha=0.1)) # Puedes experimentar con alpha\n",
    "])\n",
    "\n",
    "# Ajustar el pipeline completo con los datos de entrenamiento\n",
    "poly_ridge_pipeline.fit(x_train_pr, y_train_pr)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_test_poly_ridge = poly_ridge_pipeline.predict(x_test_pr)\n",
    "\n",
    "# Calcular métricas de evaluación EN EL CONJUNTO DE PRUEBA\n",
    "r2_test_poly_ridge = poly_ridge_pipeline.score(x_test_pr, y_test_pr) # R² en datos de prueba\n",
    "mse_test_poly_ridge = mean_squared_error(y_test_pr, y_pred_test_poly_ridge)\n",
    "mae_test_poly_ridge = mean_absolute_error(y_test_pr, y_pred_test_poly_ridge)\n",
    "rmse_test_poly_ridge = np.sqrt(mse_test_poly_ridge)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"Regresión Ridge Polinómica (Grado 2, alpha=0.1) con Pipeline y división entrenamiento/prueba\")\n",
    "print(f\"Número de muestras de entrenamiento: {x_train_pr.shape[0]}\")\n",
    "print(f\"Número de muestras de prueba: {x_test_pr.shape[0]}\")\n",
    "print(f\"\\nR² en datos de prueba: {r2_test_poly_ridge:.4f}\")\n",
    "print(f\"MSE en datos de prueba: {mse_test_poly_ridge:.2f}\")\n",
    "print(f\"RMSE en datos de prueba: {rmse_test_poly_ridge:.2f}\")\n",
    "print(f\"MAE en datos de prueba: {mae_test_poly_ridge:.2f}\")\n",
    "\n",
    "# El R² original en el test era ~0.70."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711f0ba-8361-4ebb-9b7a-e85c3ede773e",
   "metadata": {},
   "source": [
    "### Regresión Ridge Polinómica con Pipeline y `train_test_split` - Explicación\n",
    "\n",
    "Esta celda combina la potencia de la regresión polinómica para capturar relaciones no lineales con la robustez de la Regresión Ridge para prevenir el sobreajuste, todo dentro de un `Pipeline` para un manejo adecuado de los datos de entrenamiento y prueba.\n",
    "\n",
    "1.  **Preparación de Datos:**\n",
    "    *   Se utilizan las mismas características (`features_multiple` de `df`) que en los modelos anteriores.\n",
    "    *   Se dividen los datos en conjuntos de entrenamiento (80%) y prueba (20%) usando `train_test_split` con `random_state=42` para reproducibilidad.\n",
    "\n",
    "2.  **Creación del Pipeline (`poly_ridge_pipeline`):**\n",
    "    El pipeline define la secuencia de operaciones:\n",
    "    *   **`('scale', StandardScaler())`**: Primero, las características del conjunto de entrenamiento se escalan (se aprende la media y desviación estándar) y luego se transforman. Esta misma transformación (usando los parámetros aprendidos del entrenamiento) se aplicará al conjunto de prueba.\n",
    "    *   **`('polynomial', PolynomialFeatures(degree=2, include_bias=False))`**: Luego, a las características escaladas se les aplica la transformación polinómica de grado 2. De nuevo, `PolynomialFeatures` se ajusta (`fit`) solo con los datos de entrenamiento escalados y luego transforma (`transform`) tanto el entrenamiento como la prueba.\n",
    "    *   **`('ridge_model', Ridge(alpha=0.1))`**: Finalmente, el modelo de Regresión Ridge se ajusta utilizando las características escaladas y polinómicamente transformadas del conjunto de entrenamiento. `alpha=0.1` es el parámetro de regularización.\n",
    "\n",
    "3.  **Ajuste y Evaluación:**\n",
    "    *   `poly_ridge_pipeline.fit(x_train_pr, y_train_pr)`: Se ajusta el pipeline completo. Los datos de entrenamiento pasan por todos los pasos.\n",
    "    *   `poly_ridge_pipeline.predict(x_test_pr)` y `poly_ridge_pipeline.score(x_test_pr, y_test_pr)`: Para realizar predicciones y calcular el R² en el conjunto de prueba. El pipeline automáticamente aplica los pasos de `scale` y `polynomial` (usando los parámetros aprendidos del entrenamiento) a `x_test_pr` antes de alimentar los datos al modelo `Ridge`.\n",
    "    *   Se calculan las métricas MSE, RMSE y MAE en el conjunto de prueba.\n",
    "\n",
    "**Ventajas de Usar un Pipeline Aquí:**\n",
    "*   **Previene la Fuga de Datos (Data Leakage):** Asegura que el escalador y `PolynomialFeatures` se ajusten *solo* con los datos de entrenamiento y que la misma transformación se aplique consistentemente a los datos de prueba. Hacer esto manualmente es propenso a errores.\n",
    "*   **Simplifica el Código:** Encapsula todo el flujo de trabajo de preprocesamiento y modelado.\n",
    "*   **Facilita la Experimentación:** Es fácil cambiar parámetros o reemplazar pasos en el pipeline.\n",
    "\n",
    "**Interpretación Esperada:**\n",
    "*   **R² en el conjunto de prueba:** Esta es la métrica clave (originalmente ~0.70). Debería ser una buena indicación del rendimiento del modelo en datos no vistos. Compararlo con el R² de la Regresión Polinómica simple (Celda 18, que se evaluó en todo el dataset) y con el Ridge simple (Celda 19, evaluado en prueba) nos dirá si la combinación de polinomial y Ridge ofrece beneficios.\n",
    "*   Generalmente, la regresión polinómica puede mejorar el R² respecto a la lineal, pero la regularización Ridge es importante para controlar el sobreajuste que `PolynomialFeatures` puede inducir, especialmente con muchas características o un grado alto.\n",
    "\n",
    "Este enfoque es una práctica estándar y robusta en machine learning para construir modelos más complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71add191-c65b-44f5-b4a7-3f900ef13108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 21: Validación Cruzada para el Modelo Ridge Polinómico\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Usaremos el mismo pipeline que antes para consistencia\n",
    "# poly_ridge_pipeline ya está definido y ajustado, pero para CV se reajusta en cada fold\n",
    "\n",
    "# Definimos X e Y completos de nuevo, ya que cross_val_score hace su propia división\n",
    "X_cv = df[features_multiple]\n",
    "Y_cv = df['median_house_value']\n",
    "\n",
    "# Realizar validación cruzada (ej. 5 folds)\n",
    "# 'neg_mean_squared_error' es una métrica común, su negativo porque CV busca maximizar.\n",
    "# Tomaremos el R² directamente.\n",
    "cv_scores_r2 = cross_val_score(poly_ridge_pipeline, X_cv, Y_cv, cv=5, scoring='r2')\n",
    "cv_scores_neg_mse = cross_val_score(poly_ridge_pipeline, X_cv, Y_cv, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"Validación Cruzada (5-folds) para Regresión Ridge Polinómica\")\n",
    "print(f\"\\nPuntuaciones R² por fold: {cv_scores_r2}\")\n",
    "print(f\"R² Promedio: {cv_scores_r2.mean():.4f}\")\n",
    "print(f\"Desviación Estándar de R²: {cv_scores_r2.std():.4f}\")\n",
    "\n",
    "cv_scores_rmse = np.sqrt(-cv_scores_neg_mse) # Convertir MSE negativo a RMSE positivo\n",
    "print(f\"\\nPuntuaciones RMSE por fold: {cv_scores_rmse}\")\n",
    "print(f\"RMSE Promedio: {cv_scores_rmse.mean():.2f}\")\n",
    "print(f\"Desviación Estándar de RMSE: {cv_scores_rmse.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c502f9-10e9-4243-ab2f-4490c2c2545b",
   "metadata": {},
   "source": [
    "### Validación Cruzada para el Modelo Ridge Polinómico - Explicación\n",
    "\n",
    "La validación cruzada (Cross-Validation, CV) es una técnica más robusta para evaluar el rendimiento de un modelo que una simple división entrenamiento/prueba. Ayuda a obtener una estimación menos sesgada de cómo el modelo generalizará a datos independientes.\n",
    "\n",
    "1.  **Concepto de Validación Cruzada (k-Fold):**\n",
    "    *   El conjunto de datos se divide en `k` subconjuntos (o \"folds\") de aproximadamente el mismo tamaño.\n",
    "    *   El modelo se entrena `k` veces. En cada iteración, un fold diferente se usa como conjunto de validación (prueba) y los `k-1` folds restantes se usan como conjunto de entrenamiento.\n",
    "    *   Se calcula una métrica de rendimiento (ej. R², RMSE) para cada uno de los `k` folds.\n",
    "    *   El rendimiento final del modelo se promedia sobre los `k` folds.\n",
    "\n",
    "2.  **Implementación con `cross_val_score`:**\n",
    "    *   `cross_val_score(poly_ridge_pipeline, X_cv, Y_cv, cv=5, scoring='r2')`:\n",
    "        *   `poly_ridge_pipeline`: El estimador (nuestro pipeline completo) que se va a evaluar.\n",
    "        *   `X_cv`, `Y_cv`: Las características y la variable objetivo completas. `cross_val_score` se encarga internamente de las divisiones.\n",
    "        *   `cv=5`: Especifica que se use validación cruzada de 5 folds.\n",
    "        *   `scoring='r2'`: La métrica que se usará para evaluar el rendimiento en cada fold. También se calcula con `neg_mean_squared_error` para obtener el RMSE.\n",
    "\n",
    "3.  **Resultados:**\n",
    "    *   **Puntuaciones por Fold:** Se muestra el R² y el RMSE obtenidos en cada uno de los 5 folds.\n",
    "    *   **Promedio:** El R² promedio y el RMSE promedio dan una estimación más estable del rendimiento del modelo.\n",
    "    *   **Desviación Estándar:** La desviación estándar de las puntuaciones indica la variabilidad del rendimiento del modelo a través de diferentes subconjuntos de datos. Una desviación estándar baja sugiere que el modelo es estable.\n",
    "\n",
    "**Beneficios de la Validación Cruzada:**\n",
    "*   **Estimación de Rendimiento Más Fiable:** Reduce la varianza asociada con una única división entrenamiento/prueba.\n",
    "*   **Mejor Uso de los Datos:** Cada punto de dato se usa tanto para entrenamiento como para validación exactamente una vez.\n",
    "*   **Detección de Sobreajuste:** Si el rendimiento en los folds de CV es consistentemente peor que el rendimiento en el conjunto de entrenamiento completo, puede indicar sobreajuste.\n",
    "\n",
    "El R² promedio y el RMSE promedio de la validación cruzada (~0.72 y ~200k respectivamente, pueden variar ligeramente) deberían ser similares al R² y RMSE obtenidos en el conjunto de prueba de la celda anterior si el modelo es estable. Esta es una buena práctica para reportar el rendimiento de un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c07635-4547-4eab-a46a-21acc4c0f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda de Hiperparámetros para Ridge (alpha) usando GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Usaremos un pipeline similar, pero sin el modelo Ridge final para GridSearchCV\n",
    "# ya que GridSearchCV lo añadirá con diferentes alphas.\n",
    "# O podemos pasar el pipeline completo y ajustar 'ridge_model__alpha'.\n",
    "\n",
    "\n",
    "\n",
    "# Usaremos el pipeline completo (más limpio y recomendado)\n",
    "# Definir el espacio de parámetros para 'alpha' en el paso 'ridge_model' del pipeline\n",
    "param_grid_ridge = {\n",
    "    'ridge_model__alpha': [0.01, 0.1, 0.5, 1, 5, 10, 50, 100] # Valores de alpha a probar\n",
    "}\n",
    "\n",
    "# Crear el objeto GridSearchCV\n",
    "# poly_ridge_pipeline ya está definido\n",
    "# Usaremos los datos de entrenamiento/prueba ya divididos (x_train_pr, y_train_pr)\n",
    "# GridSearchCV realizará validación cruzada internamente sobre x_train_pr\n",
    "grid_search_ridge = GridSearchCV(estimator=poly_ridge_pipeline, \n",
    "                                 param_grid=param_grid_ridge, \n",
    "                                 cv=3, # 3-fold CV para la búsqueda (más rápido que 5 para este ejemplo)\n",
    "                                 scoring='r2',\n",
    "                                 verbose=1) # Muestra progreso\n",
    "\n",
    "# Ajustar GridSearchCV a los datos de entrenamiento\n",
    "print(\"Iniciando GridSearchCV para Ridge alpha...\")\n",
    "grid_search_ridge.fit(x_train_pr, y_train_pr)\n",
    "\n",
    "# Mostrar los mejores parámetros y la mejor puntuación\n",
    "print(f\"\\nMejor valor de alpha encontrado: {grid_search_ridge.best_params_['ridge_model__alpha']}\")\n",
    "print(f\"Mejor puntuación R² (CV en entrenamiento): {grid_search_ridge.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar el mejor modelo encontrado por GridSearchCV en el conjunto de prueba\n",
    "best_ridge_model = grid_search_ridge.best_estimator_\n",
    "y_pred_test_best_ridge = best_ridge_model.predict(x_test_pr)\n",
    "\n",
    "r2_test_best_ridge = best_ridge_model.score(x_test_pr, y_test_pr)\n",
    "mse_test_best_ridge = mean_squared_error(y_test_pr, y_pred_test_best_ridge)\n",
    "rmse_test_best_ridge = np.sqrt(mse_test_best_ridge)\n",
    "mae_test_best_ridge = mean_absolute_error(y_test_pr, y_pred_test_best_ridge)\n",
    "\n",
    "print(f\"\\nEvaluación del mejor modelo Ridge en el CONJUNTO DE PRUEBA:\")\n",
    "print(f\"R² en datos de prueba: {r2_test_best_ridge:.4f}\")\n",
    "print(f\"MSE en datos de prueba: {mse_test_best_ridge:.2f}\")\n",
    "print(f\"RMSE en datos de prueba: {rmse_test_best_ridge:.2f}\")\n",
    "print(f\"MAE en datos de prueba: {mae_test_best_ridge:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052ab25-62cc-465d-be0b-fe2a4f8cc15d",
   "metadata": {},
   "source": [
    "### Búsqueda de Hiperparámetros para Ridge (`alpha`) usando `GridSearchCV` - Explicación\n",
    "\n",
    "Los modelos de machine learning a menudo tienen \"hiperparámetros\" que no se aprenden directamente de los datos, sino que se deben configurar antes del entrenamiento (ej. el `alpha` en Ridge Regression, el `degree` en `PolynomialFeatures`). Encontrar los mejores hiperparámetros puede mejorar significativamente el rendimiento del modelo. `GridSearchCV` es una técnica para automatizar esta búsqueda.\n",
    "\n",
    "1.  **Concepto de `GridSearchCV`:**\n",
    "    *   Se define una \"rejilla\" (grid) de valores de hiperparámetros que se quieren probar.\n",
    "    *   `GridSearchCV` entrena y evalúa el modelo (usando validación cruzada) para cada combinación de hiperparámetros en la rejilla.\n",
    "    *   Finalmente, selecciona la combinación de hiperparámetros que dio el mejor rendimiento según la métrica especificada.\n",
    "\n",
    "2.  **Implementación:**\n",
    "    *   **`param_grid_ridge`**: Se define un diccionario donde las claves son los nombres de los hiperparámetros a ajustar (en formato `nombrepasopipeline__parametro`) y los valores son listas de los valores a probar. Aquí, probamos varios valores para `alpha` del modelo `Ridge` dentro de nuestro `poly_ridge_pipeline`.\n",
    "    *   **`GridSearchCV(...)`**:\n",
    "        *   `estimator=poly_ridge_pipeline`: El modelo (o pipeline) base cuyos hiperparámetros queremos optimizar.\n",
    "        *   `param_grid=param_grid_ridge`: La rejilla de hiperparámetros.\n",
    "        *   `cv=3`: Especifica validación cruzada de 3 folds para cada combinación de parámetros. Se usa 3 en lugar de 5 para acelerar la demostración; en un proyecto real, 5 o 10 es común.\n",
    "        *   `scoring='r2'`: La métrica para seleccionar el mejor modelo.\n",
    "        *   `verbose=1`: Muestra mensajes sobre el progreso de la búsqueda.\n",
    "    *   **`grid_search_ridge.fit(x_train_pr, y_train_pr)`**: Se ajusta `GridSearchCV` al conjunto de entrenamiento. Esto implica entrenar y validar múltiples modelos.\n",
    "\n",
    "3.  **Resultados de la Búsqueda:**\n",
    "    *   **`grid_search_ridge.best_params_`**: Muestra el valor de `alpha` (y otros parámetros del pipeline si se hubieran incluido en la búsqueda) que resultó en el mejor rendimiento.\n",
    "    *   **`grid_search_ridge.best_score_`**: La puntuación R² promedio (de la validación cruzada interna en el conjunto de entrenamiento) obtenida con los mejores parámetros.\n",
    "\n",
    "4.  **Evaluación del Mejor Modelo en el Conjunto de Prueba:**\n",
    "    *   **`best_ridge_model = grid_search_ridge.best_estimator_`**: `GridSearchCV` automáticamente re-entrena un modelo final con los mejores parámetros encontrados, utilizando *todo* el conjunto de entrenamiento `x_train_pr`. Este es el `best_estimator_`.\n",
    "    *   Este `best_ridge_model` (que es un pipeline) se evalúa luego en el conjunto de prueba `x_test_pr` para obtener una estimación final de su rendimiento en datos no vistos.\n",
    "\n",
    "**Beneficios:**\n",
    "*   **Optimización del Modelo:** Ayuda a encontrar una configuración de hiperparámetros que puede llevar a un mejor rendimiento predictivo.\n",
    "*   **Proceso Sistemático:** Evita la prueba manual y error de diferentes valores de hiperparámetros.\n",
    "\n",
    "El R² en el conjunto de prueba con el `alpha` optimizado podría ser ligeramente diferente (ojalá mejor o más estable) que el obtenido con un `alpha` elegido arbitrariamente. Este proceso demuestra una práctica más avanzada y rigurosa en la construcción de modelos. El valor de `alpha` óptimo puede variar; un `alpha` muy grande podría llevar a un \"underfitting\" si penaliza demasiado los coeficientes, mientras que un `alpha` muy pequeño se parecería a una regresión polinómica sin regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97989fb7-096a-4626-8d3e-9afd021a081c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
